# =============================================================================
# LLM Configuration - Defuse 2.O
# =============================================================================
# Central config to control which LLM model is used for each processing task.
# Change any model value below to reassign a task to a different LLM.
#
# Available Models:
#   - vertex_ai.gemini-2.0-flash        (fast, cost-effective)
#   - vertex_ai.gemini-2.0-flash-001    (pinned version of above)
#   - azure.gpt-4o                      (strong reasoning, higher cost)
#   - vertex_ai.anthropic.claude-sonnet-4  (strong coding & analysis)
#   - bedrock.anthropic.claude-sonnet-4    (same model via AWS Bedrock)
# =============================================================================

defaults:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 6096
  timeout: 180

# ---------------------------------------------------------------------------
# Repository Analysis & Documentation
# ---------------------------------------------------------------------------
repo_analysis:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 6096

documentation_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 6096

# ---------------------------------------------------------------------------
# BRD & Business Artifacts
# ---------------------------------------------------------------------------
brd_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 8192

bpmn_diagram:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 6096

user_story_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 8192

copilot_prompt:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 8192

related_stories:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 4096

# ---------------------------------------------------------------------------
# Test Generation
# ---------------------------------------------------------------------------
test_case_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 8192

test_data_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 6096

# ---------------------------------------------------------------------------
# JIRA Agent
# ---------------------------------------------------------------------------
jira_agent:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 6096

jira_intent_extraction:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 500

jira_ticket_extraction:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 500

jira_search:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 500

jira_knowledge_base:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 4096

jira_context_enrichment:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 500

jira_description_enhance:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 1500

jira_response_format:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 2000

# ---------------------------------------------------------------------------
# Unit Test Agent
# ---------------------------------------------------------------------------
unit_test_analysis:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 1000

unit_test_coverage:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 2000

unit_test_discovery:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 2000

unit_test_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.1
  max_tokens: 8192

unit_test_fix:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.15
  max_tokens: 8192

# ---------------------------------------------------------------------------
# Code Generation Agent
# ---------------------------------------------------------------------------
code_gen_plan:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 6096

code_gen_implementation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 6096

# ---------------------------------------------------------------------------
# Security Agent (Shannon)
# ---------------------------------------------------------------------------
security_analysis:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.2
  max_tokens: 6096

security_assessment:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.5
  max_tokens: 6096

# ---------------------------------------------------------------------------
# Web Test Agent
# ---------------------------------------------------------------------------
web_test_generation:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.3
  max_tokens: 8192

web_test_analysis:
  model: vertex_ai.gemini-2.0-flash
  temperature: 0.4
  max_tokens: 8192
