# Unit Test Agent Prompts
# These prompts are used by the Unit Test Agent for generating and fixing unit tests

tech_stack_detection: |
  Analyze this project's configuration to determine the tech stack and test file placement strategy.

  {structure_info}

  Configuration files:
  {config_text}

  Analyze and return ONLY a JSON object with these fields:
  {{
    "framework": "react" | "next.js" | "vue" | "angular" | "node.js" | "express" | "fastapi" | "django" | "flask" | null,
    "test_framework": "jest" | "vitest" | "react-testing-library" | "pytest" | "unittest" | null,
    "test_location": "next-to-source" | "separate-dir" | "mirror-structure",
    "import_style": "relative" | "absolute" | "alias",
    "path_alias": "@" | "~" | null (if using path aliases like @/components),
    "is_react": boolean,
    "is_nextjs": boolean,
    "is_vue": boolean,
    "is_angular": boolean,
    "has_src_dir": boolean,
    "reasoning": "brief explanation"
  }}

  Guidelines:
  - "next-to-source": place test files next to source files (common in React/Next.js)
  - "separate-dir": place in dedicated tests directory (common in Python)
  - "mirror-structure": mirror source structure in __tests__ directory
  - For React/Next.js apps with src directory, prefer "next-to-source"
  - Check package.json dependencies for framework detection
  - Check for @/ or ~/ aliases in tsconfig or jsconfig

  Return ONLY the JSON, no markdown or explanations.

test_pattern_analysis: |
  You are a senior {language} test engineer. Analyze these existing test files and extract the testing patterns, conventions, and style used in this project.

  {samples_text}

  Provide a concise style guide covering:
  1. Test framework and assertion library used
  2. Naming conventions (test function/method names, describe blocks)
  3. Import/require patterns
  4. Setup/teardown patterns (fixtures, beforeEach, setUp, etc.)
  5. Mocking approach (what library, how mocks are created)
  6. File organization (grouping, nesting, describe blocks)
  7. Any custom utilities or helpers used
  8. Code style (indentation, quotes, semicolons for JS/TS)

  Return ONLY the style guide as plain text, no markdown headers. Be concise and specific.

test_gap_identification: |
  You are a senior software engineer analyzing a {language} codebase for test coverage gaps.

  Given these source files (with their current test coverage status), identify the modules that need tests.

  PRIORITIZATION RULES:
  1. Files marked [ NO TESTS] should be HIGH priority - they have zero test coverage
  2. Files marked [HAS TESTS] should be MEDIUM priority - they may need additional coverage for untested functions
  3. Skip configuration files, entry points (main.py, index.js, app.py), migration files, and auto-generated code
  4. Focus on files with business logic, utility functions, services, and core functionality

  Source files:
  {file_list}

  Return a JSON array of objects, each with:
  - "file": the file path
  - "priority": "high", "medium", or "low"
  - "reason": one-line explanation
  - "mode": "new" if [NO TESTS], "augment" if [HAS TESTS]

  Return ONLY the JSON array, no other text. Limit to the top 10 most important files.

remove_failing_tests: |
  You are a senior {language} developer. A test file has some passing tests and some failing tests.
  Your task: REMOVE the failing tests and KEEP ONLY the passing tests. Return a valid, runnable test file.

  SOURCE FILE ({filepath}):
  ```{language}
  {source_content}
  ```

  CURRENT TEST CODE (has some failing tests):
  ```{language}
  {test_code}
  ```

  TEST ERROR OUTPUT (shows which tests fail and why):
  ```
  {error_output}
  ```

  RULES:
  1. Carefully read the error output to identify EXACTLY which test functions/cases FAIL
  2. REMOVE every failing test function/case entirely
  3. KEEP all passing tests exactly as they are - do not modify them
  4. KEEP all necessary imports, setup, and teardown that the passing tests need
  5. If ALL tests are failing, return ONLY the imports and an empty test class/describe block with a single trivial test that just asserts true
  6. The result must be a complete, runnable test file
  7. DO NOT add new tests - only keep existing passing ones

  Return ONLY the complete test file code with only passing tests. No explanations, no markdown fences.

fix_failing_tests: |
  You are a senior {language} developer fixing failing unit tests. This is fix attempt {attempt}/{max_attempts}.

  ğŸ¯ **YOUR TASK:** Analyze the TEST EXECUTION ERROR OUTPUT below and fix the FAILING TEST CODE to make it pass.

  **CRITICAL:** You have access to three pieces of information:
  1. The SOURCE FILE (what the tests are testing)
  2. The FAILING TEST CODE (the generated test that's broken)
  3. The TEST EXECUTION ERROR OUTPUT (the console output showing exactly what's wrong)

  Your job is to READ THE ERROR OUTPUT carefully, identify what's broken in the test code, and fix it.

  SOURCE FILE ({filepath}):
  ```{language}
  {source_content}
  ```

  âŒ FAILING TEST CODE (THIS IS WHAT YOU NEED TO FIX):
  ```{language}
  {test_code}
  ```

  ğŸš¨ TEST EXECUTION ERROR OUTPUT (THIS TELLS YOU WHAT'S WRONG):
  ```
  {error_output}
  ```

  {deps_context_section}
  {cra_note}

  ğŸ“‹ **STEP-BY-STEP FIXING PROCESS:**

  Step 1: READ the "TEST EXECUTION ERROR OUTPUT" above - this shows EXACTLY what failed
  Step 2: IDENTIFY the root cause (import error? mock issue? wrong assertion? wrong function signature?)
  Step 3: LOCATE the broken code in the "FAILING TEST CODE"
  Step 4: FIX the specific issue (update import path, fix mock, correct assertion, etc.)
  Step 5: VERIFY your fix addresses the error message
  Step 6: Return the COMPLETE fixed test file

  âš ï¸ The error output is your guide - use it to pinpoint exactly what needs fixing!

  DEBUG CHECKLIST - Fix these common errors:

  1. **ImportError/ModuleNotFoundError**: 
     - Verify import paths match the actual file structure  
     - Mock modules that aren't available in test environment
     - For React: ensure test file can import component correctly

  2. **AttributeError** (function/class doesn't exist):
     - Re-check the source file - test ONLY what actually exists
     - Don't assume methods/properties that aren't in source
     - Remove tests for non-existent functionality

  3. **TypeError** (wrong arguments):  
     - Match EXACT function signature from source
     - Check for default parameters, optional args, *args, **kwargs
     - Verify you're passing correct number and type of arguments

  4. **AssertionError** (wrong expected values):
     - Re-analyze source code logic carefully
     - Use realistic test data that matches actual behavior
     - Don't guess return values - deduce from source code

  5. **Cannot find module** (JS/TS):
     - Fix relative import path from test file to source file
     - Use correct path separators and extensions

  6. **Mock issues**:
     {mocking_guidance}
     - Mock external dependencies BEFORE they're imported/used
     - For Python: use unittest.mock.patch or pytest-mock
     - For JS/TS: use jest.mock() at top level

  ğŸ¯ **ERROR ANALYSIS - This test has a {error_type} error:**
  {error_specific_guidance}

  FIX STRATEGY FOR ATTEMPT {attempt}:
  {fix_strategy_note}
  {focus_note}
  {better_less_note}

  OUTPUT REQUIREMENTS:
  - Return the COMPLETE fixed test file
  - Every test MUST pass when run
  - Remove any test that you cannot fix with 100% confidence
  - NO explanations, NO markdown fences - just the code

generate_unit_tests_augment: |
  You are a senior {language} developer improving test coverage.

  This source file already has tests. Your job is to INCREASE COVERAGE by adding ONLY NEW test cases that are NOT already covered.

  Source file: {filepath}
  ```{language}
  {content}
  ```

  Existing test file:
  ```{language}
  {existing_test_content}
  ```
  {style_section}{deps_section}{imports_section}{import_section}
  CRITICAL RULES:
  1. Analyze the existing tests carefully - understand what is ALREADY tested
  2. Identify functions, methods, branches, and edge cases that are NOT covered
  3. Generate ONLY the new test functions/methods that need to be ADDED
  4. DO NOT duplicate any existing test - no overlapping test names or scenarios
  5. Follow the EXACT same coding style, naming conventions, and patterns as the existing tests
  6. Follow the import guidelines above - use correct import paths based on project structure
  7. Focus on: untested functions, missing edge cases, error paths, boundary conditions
  8. MOCK all external dependencies (database, API calls, file I/O, network) - do NOT rely on real connections
  9. Make sure every test can run independently without any external service

  Return the COMPLETE UPDATED test file that includes both the existing tests AND the new tests integrated together.
  The code must be ready to save to a file and run. Return ONLY the code, no explanations or markdown.

generate_unit_tests_new: |
  You are a senior {language} developer writing comprehensive unit tests that MUST PASS on first run.

  Analyze this source file and generate thorough unit tests using {framework}.

  File: {filepath}
  ```{language}
  {content}
  ```
  {style_section}{deps_section}{imports_section}{import_section}

  CRITICAL REQUIREMENTS (tests MUST pass validation):
  1. âœ… Write tests ONLY for functions/classes that ACTUALLY EXIST in the source file above
  2. âœ… Match function signatures EXACTLY - check parameter names, count, and default values
  3. âœ… MOCK ALL external dependencies (API calls, database, file I/O, network requests) - tests MUST run without any external service
  4. âœ… For JavaScript/TypeScript: Place ALL jest.mock() calls at the TOP of the file, BEFORE any imports
  5. âœ… For imports: Use RELATIVE paths (e.g., './Component' or '../utils/helper') - verify the path is correct for where the test file will be located
  6. âœ… Add descriptive test names that explain what is being tested
  7. âœ… Group related tests in test classes/describe blocks where appropriate
  8. âœ… Include positive tests (happy path), negative tests (error cases), and edge cases
  9. âœ… Every test must be self-contained and deterministic - no reliance on external state
  10. âœ… Follow {framework} best practices and conventions

  ğŸš¨ COMMON MISTAKES TO AVOID (90% of test failures come from these):

  **1. IMPORT ERRORS (40-50% of failures):**
  âŒ Wrong: import {{ Component }} from '/absolute/path'  (absolute paths fail)
  âŒ Wrong: import {{ Component }} from 'Component'  (missing ./ or ../)
  âœ… Correct: import {{ Component }} from './Component'  (relative path)
  âœ… Correct: import {{ Component }} from '../components/Component'  (relative with directory)

  **2. MOCK ERRORS (20-30% of failures):**
  âŒ Wrong order:
  ```javascript
  import axios from 'axios';  // âŒ import before mock
  jest.mock('axios');  // âŒ too late!
  ```
  âœ… Correct order:
  ```javascript
  jest.mock('axios');  // âœ… mock FIRST
  jest.mock('../services/api');  // âœ… before imports
  import axios from 'axios';  // âœ… then import
  ```

  **3. TESTING NON-EXISTENT CODE (15-20% of failures):**
  âŒ Testing: APIService.getUser(id)  when source only has fetchUser(id)
  âŒ Testing: Component.handleSubmit()  when source has no such method
  âœ… Only test what you see in the source code above!

  **4. ASYNC HANDLING (10-15% of failures):**
  âŒ Wrong: it('fetches data', () => {{ fetchData(); expect(...) }})  // missing async/await
  âœ… Correct: it('fetches data', async () => {{ await fetchData(); expect(...) }})

  **5. WRONG FUNCTION SIGNATURES (10% of failures):**
  âŒ Testing: calculate(a, b, c)  when function only takes (a, b)
  âœ… Match the EXACT number and names of parameters from source

  PRE-FLIGHT CHECKLIST (before generating each test):
  â˜‘ï¸ Does this function/class exist in the source file? (scroll up and verify!)
  â˜‘ï¸ Does my import path account for the test file location?
  â˜‘ï¸ Are all mocks defined at the TOP of the file?
  â˜‘ï¸ Am I testing behavior that can be determined from the source code?
  â˜‘ï¸ Have I mocked all external dependencies (API, database, localStorage, etc.)?

  Return ONLY the complete test file code, no explanations or markdown. The code must be ready to save to a file and run.
